{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a67cba0",
   "metadata": {},
   "source": [
    "## Import Libralies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c9ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libralies\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==============================================================================\n",
    "## 1. Read and Explore Data\n",
    "# ==============================================================================\n",
    "\n",
    "def read_datafile(filepath):\n",
    "    \"\"\"reads the data file and pastes into a data frame.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df\n",
    "\n",
    "filepath = 'Wholesale customers data.csv'\n",
    "df = read_datafile(filepath)\n",
    "\n",
    "print(\"Data Overview:\")\n",
    "df.info()\n",
    "\n",
    "print(f\"\\nData Shape: {df.shape}\")\n",
    "print(f\"Null Values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate Rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# ==============================================================================\n",
    "## 2. Exploratory Data Analysis (EDA)\n",
    "# ==============================================================================\n",
    "\n",
    "# Create a list of the 6 spending features\n",
    "spending_features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "\n",
    "# Create a figure with a 2x3 grid of subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each spending feature and plot its distribution by Channel\n",
    "for i, feature in enumerate(spending_features):\n",
    "    sns.boxplot(x='Channel', y=feature, data=df, ax=axes[i], palette='Set2', hue='Channel', legend=False)\n",
    "    axes[i].set_title(f'Spending Distribution of {feature} by Channel')\n",
    "    axes[i].set_xlabel('Channel (1=Horeca, 2=Retail)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "## 3. Data Preprocessing\n",
    "# ==============================================================================\n",
    "\n",
    "## 3.1. Log Transformation\n",
    "spending_features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "df_log = df.copy()\n",
    "\n",
    "# Apply log(x+1) transformation\n",
    "df_log[spending_features] = np.log1p(df_log[spending_features])\n",
    "\n",
    "## 3.2. Standard Scaling and Encoding\n",
    "# One Hot Encoding for 'Channel' and 'Region'\n",
    "df_encoded = pd.get_dummies(df_log, columns=['Channel', 'Region'], drop_first=False)\n",
    "X_features = df_encoded.columns\n",
    "\n",
    "# Apply Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled_array = scaler.fit_transform(df_encoded.values)\n",
    "X_scaled = pd.DataFrame(X_scaled_array, columns=df_encoded.columns)\n",
    "\n",
    "print(f\"Shape of scaled data: {X_scaled.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "## 4. K-Means Baseline Model Exploration\n",
    "# ==============================================================================\n",
    "\n",
    "## 4.1. PCA Explained Variance (Finding N=6 Baseline)\n",
    "pca_analysis = PCA(n_components=None, random_state=42)\n",
    "pca_analysis.fit(X_scaled)\n",
    "cumulative_variance = np.cumsum(pca_analysis.explained_variance_ratio_)\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.90, color='r', linestyle='-')\n",
    "plt.text(1, 0.92, '90% Cutoff', color = 'red', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the 90% variance cutoff (N=6)\n",
    "n_components_90 = np.where(cumulative_variance >= 0.90)[0][0] + 1\n",
    "print(f\"\\nTotal Features: {X_scaled.shape[1]}\")\n",
    "print(f\"Number of components needed to explain 90% variance: {n_components_90}\")\n",
    "\n",
    "# Create the N=6 baseline PCA data for k-exploration\n",
    "pca_baseline = PCA(n_components=n_components_90, random_state=42)\n",
    "X_pca_baseline = pca_baseline.fit_transform(X_scaled) \n",
    "print(f\"\\nShape of the baseline PCA data (X_pca_baseline) using N={n_components_90}: {X_pca_baseline.shape}\")\n",
    "\n",
    "\n",
    "## 4.2. K-Means: Find Optimal k (Elbow Plot)\n",
    "k_range = range(2, 11) \n",
    "wss = [] \n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=200, random_state=42)\n",
    "    kmeans.fit(X_pca_baseline) # Using N=6 baseline data\n",
    "    wss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, wss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method for Optimal k (Using N=6 PCA Data)')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WSS)')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## 4.3. K-Means: Find Optimal k (Silhouette Score)\n",
    "silhouette_scores = {} \n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=200, random_state=42)\n",
    "    kmeans.fit(X_pca_baseline) # Using N=6 baseline data\n",
    "    score = silhouette_score(X_pca_baseline, kmeans.labels_)\n",
    "    silhouette_scores[k] = score\n",
    "\n",
    "scores_df = pd.DataFrame(list(silhouette_scores.items()), columns=['k', 'Silhouette_Score']).set_index('k')\n",
    "print(\"Baseline Silhouette Scores (N=6 Data):\")\n",
    "print(scores_df.sort_values(by='Silhouette_Score', ascending=False))\n",
    "# This confirms k=4 is the best choice.\n",
    "\n",
    "# ==============================================================================\n",
    "## 5. K-Means Model Optimization (Finding Optimal N)\n",
    "# ==============================================================================\n",
    "\n",
    "# We now test different N-components (3, 4, 6, 8, 10) to see which one\n",
    "# maximizes the Silhouette Score for our chosen k=4.\n",
    "components_to_test = [3, 4, 6, 8, 10]\n",
    "optimal_k = 4 \n",
    "component_scores = {}\n",
    "\n",
    "# Fit PCA with max components first\n",
    "pca_max = PCA(n_components=X_scaled.shape[1], random_state=42)\n",
    "X_pca_full = pca_max.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\n--- Optimizing N_Components for k={optimal_k} ---\")\n",
    "for n_comp in components_to_test:\n",
    "    X_subset = X_pca_full[:, :n_comp]\n",
    "    kmeans = KMeans(n_clusters=optimal_k, init='k-means++', n_init=200, random_state=42)\n",
    "    kmeans.fit(X_subset)\n",
    "    score = silhouette_score(X_subset, kmeans.labels_)\n",
    "    component_scores[n_comp] = score\n",
    "\n",
    "comp_scores_df = pd.DataFrame(list(component_scores.items()), \n",
    "                         columns=['N_Components', 'Silhouette_Score']).set_index('N_Components')\n",
    "\n",
    "print(\"Silhouette Scores by Number of PCA Components:\")\n",
    "print(comp_scores_df.sort_values(by='Silhouette_Score', ascending=False))\n",
    "print(\"\\nConclusion: N=3 gives the highest Silhouette Score (0.4845).\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "## 6. Final Model 1: K-Means (N=3, k=4)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" FINAL K-MEANS MODEL (N=3, k=4)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "N_COMPONENTS_FINAL = 3\n",
    "OPTIMAL_K = 4\n",
    "\n",
    "# Apply PCA with the OPTIMAL N=3\n",
    "pca_final_optimized = PCA(n_components=N_COMPONENTS_FINAL, random_state=42)\n",
    "X_pca_final = pca_final_optimized.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "## 6.1. Fit Final K-Means Model\n",
    "kmeans = KMeans(n_clusters=OPTIMAL_K, init='k-means++', n_init=200, random_state=42)\n",
    "kmeans.fit(X_pca_final)\n",
    "y_pred = kmeans.labels_\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "df['Cluster'] = y_pred\n",
    "\n",
    "## 6.2. K-Means Performance Metrics\n",
    "final_silhouette = silhouette_score(X_pca_final, y_pred)\n",
    "true_labels_proxy = df['Channel']\n",
    "final_ari_score = adjusted_rand_score(true_labels_proxy, y_pred)\n",
    "\n",
    "print(f\"--- K-MEANS FINAL MODEL PERFORMANCE ---\")\n",
    "print(f\"Silhouette Score: {final_silhouette:.4f}\")\n",
    "print(f\"Adjusted Rand Index (ARI) Score: {final_ari_score:.4f}\")\n",
    "\n",
    "## 6.3. K-Means Cluster Interpretation (Profiling)\n",
    "cluster_analysis = df.groupby('Cluster').mean(numeric_only=True)\n",
    "print(\"\\nK-Means Cluster Analysis (Mean values in ORIGINAL CURRENCY/COUNTS):\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "## 6.4. K-Means Visualization (with Centroids)\n",
    "pca_df = pd.DataFrame(X_pca_final[:, 0:2], columns=['PC1', 'PC2'])\n",
    "pca_df['Cluster'] = y_pred\n",
    "\n",
    "# Extract and prepare Centroids\n",
    "centroids = kmeans.cluster_centers_[:, 0:2]\n",
    "centroids_df = pd.DataFrame(centroids, columns=['PC1', 'PC2'])\n",
    "centroids_df['Cluster'] = range(OPTIMAL_K) \n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    data=pca_df, \n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    hue='Cluster', \n",
    "    palette='Set1', \n",
    "    style='Cluster',\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    legend=False\n",
    ")\n",
    "sns.scatterplot(\n",
    "    data=centroids_df,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    hue='Cluster',\n",
    "    palette='Set1',\n",
    "    marker='X',\n",
    "    s=200, \n",
    "    edgecolor='black',\n",
    "    linewidth=1.5,\n",
    "    legend='full'\n",
    ")\n",
    "plt.title(f'K-Means Customer Segments with Centroids (N_Components={N_COMPONENTS_FINAL})')\n",
    "plt.xlabel('Principal Component 1 (PC1)')\n",
    "plt.ylabel('Principal Component 2 (PC2)')\n",
    "plt.legend(title='Cluster Centroid', loc='upper right', markerscale=1.5)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## 6.5. K-Means Intersection Matrix\n",
    "cm_df = pd.crosstab(index=df['Channel'], columns=df['Cluster'], \n",
    "                    rownames=['True Channel'], colnames=['Cluster Prediction'])\n",
    "\n",
    "cm_df.index = ['True: Horeca (1)', 'True: Retail (2)']\n",
    "cm_df.columns = [f'Cluster {c}' for c in cm_df.columns]\n",
    "\n",
    "print(\"\\nK-Means Final Intersection Matrix (True Channel vs. Predicted Clusters):\")\n",
    "print(cm_df)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('K-Means Intersection Matrix (Final Model)')\n",
    "plt.xlabel('K-Means Cluster Assignment')\n",
    "plt.ylabel('Customer Channel')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "## 7. Model 2: GMM Comparison (N=3)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" GMM MODEL COMPARISON (N=3)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "## 7.1. GMM: Find Optimal k (AIC/BIC)\n",
    "# We test GMM on the *same optimized N=3 data* for a fair comparison.\n",
    "k_range = range(2, 11) \n",
    "aic_scores = []\n",
    "bic_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42, n_init=10)\n",
    "    gmm.fit(X_pca_final) # Using N=3 optimized data\n",
    "    aic_scores.append(gmm.aic(X_pca_final))\n",
    "    bic_scores.append(gmm.bic(X_pca_final))\n",
    "\n",
    "# Plot the AIC/BIC curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, aic_scores, marker='o', linestyle='--', label='AIC')\n",
    "plt.plot(k_range, bic_scores, marker='s', linestyle='-', label='BIC')\n",
    "plt.title('AIC and BIC for Optimal k (Using GMM on N=3 Data)')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Score (Lower is Better)')\n",
    "plt.xticks(k_range)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# The plot shows BIC minimum at k=4.\n",
    "\n",
    "## 7.2. GMM Fit and Evaluation\n",
    "# Based on your notes and the BIC plot, k=4 is the optimal choice.\n",
    "OPTIMAL_K_GMM = 4 \n",
    "N_COMPONENTS_FINAL = 3 \n",
    "\n",
    "# Fit the Final GMM Model\n",
    "gmm = GaussianMixture(n_components=OPTIMAL_K_GMM, random_state=42, n_init=10)\n",
    "gmm.fit(X_pca_final) \n",
    "y_pred_gmm = gmm.predict(X_pca_final)\n",
    "\n",
    "# Add the GMM cluster labels to a new column\n",
    "df['Cluster_GMM'] = y_pred_gmm\n",
    "\n",
    "# GMM Performance Metrics\n",
    "final_silhouette_gmm = silhouette_score(X_pca_final, y_pred_gmm)\n",
    "final_ari_score_gmm = adjusted_rand_score(df['Channel'], y_pred_gmm)\n",
    "\n",
    "print(f\"--- GMM FINAL MODEL PERFORMANCE ---\")\n",
    "print(f\"Silhouette Score (GMM): {final_silhouette_gmm:.4f}\")\n",
    "print(f\"Adjusted Rand Index (ARI) Score (GMM): {final_ari_score_gmm:.4f}\")\n",
    "\n",
    "## 7.3. GMM Cluster Interpretation and Matrix\n",
    "# GMM Cluster Profiling\n",
    "cluster_analysis_gmm = df.groupby('Cluster_GMM').mean(numeric_only=True)\n",
    "print(\"\\nCluster Analysis (GMM Mean values):\")\n",
    "print(cluster_analysis_gmm)\n",
    "\n",
    "# GMM Intersection Matrix\n",
    "cm_df_gmm = pd.crosstab(index=df['Channel'], columns=df['Cluster_GMM'], \n",
    "                    rownames=['True Channel'], colnames=['GMM Cluster Prediction'])\n",
    "cm_df_gmm.index = ['True: Horeca (1)', 'True: Retail (2)']\n",
    "cm_df_gmm.columns = [f'Cluster {c}' for c in cm_df_gmm.columns]\n",
    "\n",
    "print(\"\\nFinal Intersection Matrix (GMM):\")\n",
    "print(cm_df_gmm)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "## 8. Final Conclusion\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"K-Means (N=3, k=4): \\t Silhouette = {final_silhouette:.4f} \\t ARI = {final_ari_score:.4f}\")\n",
    "print(f\"GMM (N=3, k=4): \\t Silhouette = {final_silhouette_gmm:.4f} \\t ARI = {final_ari_score_gmm:.4f}\")\n",
    "print(\"\\nConclusion: The optimized K-Means model is the superior choice, as it provides\")\n",
    "print(\"a slightly higher Silhouette Score and a stronger alignment (ARI) with the\")\n",
    "print(\"known business channels (Horeca/Retail).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
